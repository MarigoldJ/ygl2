{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210618_nlp_day7.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNt1Af0R7hsTt8cjW54vv8G",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarigoldJ/ygl2/blob/main/class/20210618_nlp_day7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiqqFQh8rjFM"
      },
      "source": [
        "# Today's Topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VMtqbY-sDns"
      },
      "source": [
        "* RNN의 문제점\n",
        "    * 기울기 소실\n",
        "    * 번역에는 사용하기 힘들다?\n",
        "        * 각 나라별 어순 정보 파악 힘들다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05_77ktatp94"
      },
      "source": [
        "# Seq2seq 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP0FX47ztsQS"
      },
      "source": [
        "## LSTM Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP2Uj4OGeEAP"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIkXifezt5nV"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    seq2seq의 encoder\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, encoder_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(encoder_units)\n",
        "        # return_sequences 매개변수를 기본값 False로 전달\n",
        "    \n",
        "    def call(self, x):      # __call__ 과 비슷한 듯\n",
        "        '''\n",
        "        x를 넣고 중간에 데이터의 shape을 디버깅\n",
        "        '''\n",
        "        print('입력할 때 shape :', x.shape)\n",
        "\n",
        "        e = self.embedding(x)\n",
        "        print(\"Embedding Layer를 거친 뒤 shape :\", e.shape)\n",
        "\n",
        "        output_v = self.lstm(e)\n",
        "        print(\"LSTM Layer를 거친 뒤 shape :\", output_v.shape)\n",
        "\n",
        "        return output_v\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfyWzV7bvNHk",
        "outputId": "875165f9-0aa1-4445-c43a-08b33fc9a9b0"
      },
      "source": [
        "vocab_size = 30000\n",
        "emb_size = 256\n",
        "lstm_size = 512\n",
        "batch_size = 1\n",
        "sample_seq_len = 3\n",
        "\n",
        "print('Vocab Size :', vocab_size)\n",
        "print('Embedding Size :', emb_size)\n",
        "print('LSTM Size :', lstm_size)\n",
        "print('Batch Size :', batch_size)\n",
        "print('Sample Sequence Length :', sample_seq_len)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size : 30000\n",
            "Embedding Size : 256\n",
            "LSTM Size : 512\n",
            "Batch Size : 1\n",
            "Sample Sequence Length : 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1-z_A15vlR2",
        "outputId": "597fb388-6b00-434a-8c95-9b5e65164c69"
      },
      "source": [
        "encoder = Encoder(vocab_size, emb_size, lstm_size)\n",
        "sample_encoder_input = tf.zeros((batch_size, sample_seq_len))\n",
        "\n",
        "sample_encoder_output = encoder(sample_input)\n",
        "# 인코더 LSTM의 최종 State (이후 컨벡스트 벡터로 사용될 예정)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력할 때 shape : (1, 3)\n",
            "Embedding Layer를 거친 뒤 shape : (1, 3, 256)\n",
            "LSTM Layer를 거친 뒤 shape : (1, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G6hD8VHwPOX"
      },
      "source": [
        "![](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-L-6.max-800x600.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsgn-HGcwPwO"
      },
      "source": [
        "## LSTM Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1gQ-uhQv-Tf"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    '''\n",
        "    seq2seq의 decoder\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, decoder_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(decoder_units, return_sequences=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "\n",
        "    def call(self, x, context_v):\n",
        "        '''\n",
        "        디코더의 입력 x와 인코더의 컨벡스트 벡터(final state)를 인자로 받음\n",
        "        중간의 데이터들의 shape을 디버깅\n",
        "        '''\n",
        "        print('입력할 때 shape :', x.shape)\n",
        "\n",
        "        e = self.embedding(x)\n",
        "        print(\"Embedding Layer를 거친 뒤 shape :\", e.shape)\n",
        "\n",
        "        # 컨벡스트 벡터의 브로드캐스팅, 확장\n",
        "        context_vh = tf.repeat(tf.expand_dims(context_v, axis=1), repeats=x.shape[1], axis=1)\n",
        "        eh = tf.concat([e, context_vh], axis=-1)\n",
        "        print('Context Vector가 합쳐진 shape :', eh.shape)\n",
        "\n",
        "        output_v = self.lstm(eh)\n",
        "        print(\"LSTM Layer를 거친 뒤 shape :\", output_v.shape)\n",
        "\n",
        "        output = self.fc(output_v)\n",
        "        print('Decoder 최종 output의 shape :', output.shape)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OF7N4UGypVh",
        "outputId": "77e4ff63-e482-4408-911a-2270ec2b6cfa"
      },
      "source": [
        "vocab_size = 30000\n",
        "emb_size = 256\n",
        "lstm_size = 512\n",
        "batch_size = 1\n",
        "sample_seq_len = 3\n",
        "\n",
        "print('Vocab Size :', vocab_size)\n",
        "print('Embedding Size :', emb_size)\n",
        "print('LSTM Size :', lstm_size)\n",
        "print('Batch Size :', batch_size)\n",
        "print('Sample Sequence Length :', sample_seq_len)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size : 30000\n",
            "Embedding Size : 256\n",
            "LSTM Size : 512\n",
            "Batch Size : 1\n",
            "Sample Sequence Length : 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmDOdHH_ytom",
        "outputId": "a54e4c85-0b60-41fb-8879-29f52c29571e"
      },
      "source": [
        "decoder = Decoder(vocab_size, emb_size, lstm_size)\n",
        "sample_decoder_input = tf.zeros((batch_size, sample_seq_len))\n",
        "\n",
        "sample_decoder_output = decoder(sample_decoder_input, sample_encoder_output)\n",
        "# Decoder.call(x, context_v)를 호출\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력할 때 shape : (1, 3)\n",
            "Embedding Layer를 거친 뒤 shape : (1, 3, 256)\n",
            "Context Vector가 합쳐진 shape : (1, 3, 768)\n",
            "LSTM Layer를 거친 뒤 shape : (1, 3, 512)\n",
            "Decoder 최종 output의 shape : (1, 3, 30000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj7RG4NK4nC8"
      },
      "source": [
        "# 어텐션 메커니즘 (Attention Mechanism)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sf4xAN84xxt"
      },
      "source": [
        "* 기존의 RNN에 기반한 seq2seq 모델의 문제점\n",
        "    1. 기억 소실, 기울기 소실\n",
        "    2. 하나의 고정된 벡터에 모든 정보를 압축하려다 보니 정보 손실이 발생\n",
        "* 어텐션 아이디어\n",
        "    * 디코더에서 출력단어를 예측하는 매 시점마다, 인코더에서의 *전체* 입력 문장을 다시 한번 참고함\n",
        "    * 전체 입력 문장을 모두 동일한 비율로 참고하는 것이 아니라, 해당 시점에 예측할 단어와 연관성 있는 단어 부분을 중점적으로 참고함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCJkJkQ-z66J",
        "outputId": "cdbf9423-cdd2-4f45-9140-ba23e21bf12f"
      },
      "source": [
        "dictionary = {'2017': 'Transformer', '2018': 'BERT'}\n",
        "\n",
        "print(dictionary['2017'])\n",
        "print(dictionary['2018'])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transformer\n",
            "BERT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu8sTLl06gRr"
      },
      "source": [
        "![](https://wikidocs.net/images/page/22893/%EC%BF%BC%EB%A6%AC.PNG)\n",
        "* Query : t 시점의 디코더 셀에서의 hidden state\n",
        "* Key : 모든 시점의 인코더 셀에서의 hidden state\n",
        "* Value : 모든 시점의 인코더 셀의 hidden state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azS6xR0E6guU"
      },
      "source": [
        "## 닷 프로덕트 어텐션 (Dot-Product Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RhA4DBV9sMLl"
      },
      "source": [
        "* Luong이 제안한 어텐션 메커니즘\n",
        "* score function이 dot product연산이므로, 이름이 dot-product attention이라 불림"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVDAjnmmClER"
      },
      "source": [
        "### 이론 설명"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u63w2r8t65-D"
      },
      "source": [
        "![](https://wikidocs.net/images/page/22893/dotproductattention1_final.PNG)\n",
        "\n",
        "그림 설명\n",
        "* 디코더에서 sos, je를 넣어 je, suis를 예측한 뒤, 세번째 단어를 예측하려는 시점\n",
        "* suis 다음 단어를 예측하기 위해서, 인코더의 hidden state들을 참조하여 다음 단어 etudiant를 예측해내는 모습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPHixQVP7jdy"
      },
      "source": [
        "#### 1.Attention Score 구하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUhOsQBD8qPa"
      },
      "source": [
        "![](https://wikidocs.net/images/page/22893/dotproductattention2_final.PNG)\n",
        "\n",
        "\n",
        "그림 설명\n",
        "* 디코더의 현재 시점 t에서의 state와, 인코더의 모든 시점에서의 state의 유사도를 점수화한다.(내적을 통해 유사도를 계산하는 듯)\n",
        "* 수식은 아래와 같다.\n",
        "$$ score(s_t, h_i) = s_t^T h_i $$\n",
        "$$ e^t = [ score(s_t, h_1), score(s_t, h_2), ..., score(s_t, h_N) ] $$\n",
        "    * $s_t$는 디코더의 현재 시점 t에서의 state (행벡터)\n",
        "    * $h_i$는 인코더의 모든 시점 중 특정 시점 i에서의 state (행벡터)\n",
        "    * $e^t$는 디코더의 현재 시점 t에서의 attention score 모음값\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URodiYSY-cdS"
      },
      "source": [
        "#### 2.Attention Distribution 구하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnXEwfUS-p7i"
      },
      "source": [
        "* 소프트맥스(softmax) 함수를 통해 어텐션 분포를 구한다\n",
        "\n",
        "![](https://wikidocs.net/images/page/22893/dotproductattention3_final.PNG)\n",
        "\n",
        "그림 설명\n",
        "* 1에서 계산한 각 시점별 Attention Score를 softmax 함수로 처리한다.\n",
        "* 수식은 아래와 같다.\n",
        "$$ a^t = softmax(e^t) $$\n",
        "    * $a^t$는 어텐션 가중치 모음값인 attention distribution\n",
        "    * $e^t$는 디코더의 현재 시점 t에서의 attention score 모음값"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAVuwZB9ASMB"
      },
      "source": [
        "#### 3.Attention Value 구하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ntOqy-ASbC"
      },
      "source": [
        "* 각 인코더의 어텐션 가중치와 은닉 상태를 가중합해서 어텐션 값(attention value)을 구한다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/22893/dotproductattention4_final.PNG)\n",
        "\n",
        "그림 설명\n",
        "* 2에서 계산한 각 시점별 가중치인 Attention distribution과, 각 시점별 state를 곱하여 Attention value를 구한다.\n",
        "    * 이 결과 값은 종합적인 정보를 담은 state라고 생각할 수 있다.\n",
        "    * 인코더에서 디코더로 전달하는 state를 만든 것이다.\n",
        "        * 기존 : 인코더의 마지막 state\n",
        "        * 현재 하는 것 : 인코더의 state를 종합적으로 고려한 새로운 state\n",
        "* 수식은 아래와 같다.\n",
        "$$ a_t = \\sum^{N}_{i=1}{a^t_i h_i} $$\n",
        "    * $a^t$는 어텐션 가중치 모음값인 attention distribution\n",
        "        * 해당 시점을 얼마나 반영할지 나타낸다고 생각하기\n",
        "    * $h_i$는 인코더의 특정 시점 i에서의 state (행벡터)\n",
        "    * $a_t$는 가중치가 반영된 state (행벡터)\n",
        "        * $h_i$ 들을 대표하는 state라고 생각하면 좋을 듯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Na9oSYASnR"
      },
      "source": [
        "#### 4.Attention value, decoder state 연결하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YdZg7liDorm"
      },
      "source": [
        "* 3에서 구한 어텐션 값과, 기존의 디코더의 현재 시점 t의 state를 연결한다.\n",
        "* 새로운 벡터를 만드는 것이다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/22893/dotproductattention5_final_final.PNG)\n",
        "\n",
        "그림 설명\n",
        "* 3에서 계산한 인코더의 종합 State인 Attention Value와, 기존의 디코더의 현재 시점 t의 state를 연결하여 새로운 벡터를 만든다.\n",
        "* 굳이 수식으로 쓰면 아래와 같다.\n",
        "$$ v_t = [a_t;s_t] $$\n",
        "    * $v_t$는 $a_t$와 $s_t$를 단순히 연결한 것. (행벡터)\n",
        "    * $a_t$는 가중치가 반영된 인코더의 state (행벡터)\n",
        "        * 인코더의 정보를 종합한 state로 보면 좋을 듯!\n",
        "    * $s_t$는 디코더의 현재 시점 t에서의 state (행벡터)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h13mALIoDl94"
      },
      "source": [
        "#### 5.출력층 연산의 입력 벡터 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0VBWO4vDpbX"
      },
      "source": [
        "* 논문 내용에 따르면, 4에서의 $v_t$가 출력층으로 가기전에 한가지 연산을 거침.\n",
        "* 연산이라 함은 $tanh$ 함수를 거치는 연산을 의미함.\n",
        "\n",
        "![](https://wikidocs.net/images/page/22893/st.PNG)\n",
        "\n",
        "그림 설명\n",
        "* 어텐션 메커니즘을 사용하기 전에는, 출력층의 입력으로 $s_t$(디코더에서 시점 t의 state) 가 사용됨.\n",
        "* 어텐션 메커니즘을 사용하는 지금은, 출력층의 입력으로 $\\tilde{s_t}$(attention이 반영된 디코더에서 시점 t의 state) 가 사용됨.\n",
        "* $\\tilde{s_t}$를 구하는 수식은 아래와 같다.\n",
        "$$ \\tilde{s_t}=tanh(W_c v_t + b_c) $$\n",
        "    * $v_t$는 $a_t$와 $s_t$를 단순히 연결한 것. (행벡터)\n",
        "    * $W_c$는 학습 가능한 가중치 행렬\n",
        "    * $b_c$는 편향\n",
        "    * $\\tilde{s_t}$는 attention이 반영된 새로운 $s_t$(디코더의 현재 시점 t에서의 state) (행벡터)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZxlXXH6DnXX"
      },
      "source": [
        "#### 6.출력층으로부터 예측 벡터 얻기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D100tnfdDqDn"
      },
      "source": [
        "* 5에서 구한 출력층의 입력벡터를 출력층에 넣고 연산하여, 예측 벡터($\\tilde{y_t}$)를 얻는다.\n",
        "\n",
        "* 수식은 아래와 같다. \n",
        "$$ \\tilde{y_t}=softmax(W_y \\tilde{s_t} + b_y) $$\n",
        "    * $\\tilde{s_t}$는 attention이 반영된 새로운 $s_t$(디코더의 현재 시점 t에서의 state) (행벡터)\n",
        "    * $W_y$는 학습 가능한 가중치 행렬\n",
        "    * $b_y$는 편향\n",
        "    * $\\tilde{y_t}$는 예측벡터 (이 값을 문자로 바꾸면 예측 문자가 된다)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfqId8aydWG5"
      },
      "source": [
        "## 바다나우 어텐션 (Bahdanau Attention, concat attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioEeqa0csYqW"
      },
      "source": [
        "* Bahdanau가 제안한 어텐션 메커니즘\n",
        "* score function이 concat 연산이라서 concat attention, 혹은 제안자 이름을 따서 Bahdanau attention이라 불림"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_hz9aUZdMN9"
      },
      "source": [
        "* attention score를 얻는 식\n",
        "$$ score_{alignment} = W * tanh(W_{decoder}*H_{decoder} + W_{encoder}*H_{encoder}) $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38s1V-2eqefQ"
      },
      "source": [
        "### 코드 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q_U1lWjCrg_"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):    \n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.w_decoder = tf.keras.layers.Dense(units)\n",
        "        self.w_encoder = tf.keras.layers.Dense(units)\n",
        "        self.w_combine = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, h_encoder, h_decoder):\n",
        "        '''\n",
        "        h_encoder : encoder hidden state\n",
        "        h_decoder : decoder hidden state\n",
        "        '''\n",
        "        wh_encoder = self.w_encoder(h_encoder)\n",
        "        wh_decoder = self.w_decoder(tf.expand_dims(h_decoder, 1))\n",
        "\n",
        "        print('[h_encoder] shape :', h_encoder.shape)\n",
        "        print('[w_encoder x h_encoder] shape :', wh_encoder.shape)\n",
        "        \n",
        "        print('[h_decoder] shape :', h_decoder.shape)\n",
        "        print('[w_decoder x h_decoder] shape :', wh_decoder.shape)\n",
        "\n",
        "        # attention score\n",
        "        at_score = self.w_combine(tf.nn.tanh(wh_decoder + wh_encoder))\n",
        "        print('[score_alignment] shape :', at_score.shape)\n",
        "\n",
        "        # attention distribution(weight)\n",
        "        at_weight = tf.nn.softmax(at_score, axis=1)\n",
        "        print('\\n최종 attention weight :', at_weight.numpy())\n",
        "\n",
        "        # attention value?(convext vector 얻기)\n",
        "        context_v = at_weight * wh_decoder\n",
        "        context_v = tf.reduce_sum(context_v, axis=1)\n",
        "\n",
        "        return context_v, at_weight\n"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXHJ4HXm6MvO",
        "outputId": "375af986-8f3a-45bd-d6f9-7dcb44b73bc9"
      },
      "source": [
        "w_size = 100\n",
        "\n",
        "print(f'Hidden State를 {w_size}차원으로 Mapping\\n')\n",
        "\n",
        "attention = BahdanauAttention(w_size)\n",
        "\n",
        "encode_state = tf.random.uniform((1, 10, 512))\n",
        "decode_state = tf.random.uniform((1, 512))\n",
        "\n",
        "_ = attention(encode_state, decode_state)\n"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Hidden State를 100차원으로 Mapping\n",
            "\n",
            "[h_encoder] shape : (1, 10, 512)\n",
            "[w_encoder x h_encoder] shape : (1, 10, 100)\n",
            "[h_decoder] shape : (1, 512)\n",
            "[w_decoder x h_decoder] shape : (1, 1, 100)\n",
            "[score_alignment] shape : (1, 10, 1)\n",
            "\n",
            "최종 attention weight : [[[0.09870458]\n",
            "  [0.13317658]\n",
            "  [0.10386166]\n",
            "  [0.1029733 ]\n",
            "  [0.09819912]\n",
            "  [0.09062405]\n",
            "  [0.07663803]\n",
            "  [0.09599996]\n",
            "  [0.07814394]\n",
            "  [0.12167887]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G06Nhl4ZDlh_"
      },
      "source": [
        "## 루옹 어텐션 (Luong Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVsyLLuxs3_2"
      },
      "source": [
        "* Luong이 제안한 어텐션 메커니즘\n",
        "* score function 이름이 general임.\n",
        "\n",
        "* dot-product attention도 Luong이 제안했지만, 지금 확인 중인 attention도 Luong이 제안함. \n",
        "    * 둘의 차이는 score function에 가중치 $W_a$ 존재 여부임."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4MbQOompOaw"
      },
      "source": [
        "* attention score를 얻는 식\n",
        "$$ score(H_{target}, H_{source}) = H_{target}^T*W_{combine}*H_{source} $$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ASheIglSs2n-"
      },
      "source": [
        "### 코드 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz2_-pIoD9ax"
      },
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.W_combine = tf.keras.layers.Dense(units)\n",
        "\n",
        "    def call(self, h_encoder, h_decoder):\n",
        "        '''\n",
        "        h_encoder : encoder hidden state (식에서 H_source)\n",
        "        h_decoder : decoder hidden state (식에서 H_target)\n",
        "        '''\n",
        "        # attention score\n",
        "        # 우선 W_combine * H_source를 수행\n",
        "        wh_combine = self.W_combine(h_encoder)  # W_combine * H_source\n",
        "\n",
        "        print('[h_source] shape :', h_encoder.shape)\n",
        "        print('[w_combine x h_source] shape :', wh_combine.shape)\n",
        "\n",
        "        # H_target.T와 W_combine * H_source의 행렬곱 수행 (=score)\n",
        "        h_decoder_rev = tf.expand_dims(h_decoder, 1)                                    # 차원 수가 달라서, 차원 확장\n",
        "        at_score_rev = tf.matmul(wh_combine, tf.transpose(h_decoder_rev, [0, 2, 1]))    # 결과값이 10개 score 값이 나오도록 행렬곱\n",
        "        \n",
        "        print('[score_alignment] shape :', at_score_rev.shape)  # (1, 10, 1)\n",
        "\n",
        "        # attention distribution(weight)\n",
        "        at_weight_rev = tf.nn.softmax(at_score_rev, axis=1)\n",
        "\n",
        "        print('\\n최종 attention weight :', at_weight_rev.numpy())\n",
        "\n",
        "        # 늘렸던 차원수 다시 맞춰주기\n",
        "        at_weight = tf.squeeze(at_weight_rev, axis=-1)          # (1, 10)\n",
        "\n",
        "        # attention value?(convext vector 얻기)\n",
        "        context_v = tf.matmul(at_weight, h_encoder)     # (1, 10) * (1, 10, 512) = (1, 512)\n",
        "\n",
        "        return context_v, at_weight\n"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ppZjyOu4W9-",
        "outputId": "997b1979-eefb-40a0-f1c3-b8ea0c936683",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "emb_dim = 512\n",
        "\n",
        "attention = LuongAttention(emb_dim)\n",
        "\n",
        "enc_state = tf.random.uniform((1, 10, emb_dim))\n",
        "dec_state = tf.random.uniform((1, emb_dim))\n",
        "\n",
        "_ = attention(enc_state, dec_state)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[h_source] shape : (1, 10, 512)\n",
            "[w_combine x h_source] shape : (1, 10, 512)\n",
            "[score_alignment] shape : (1, 10, 1)\n",
            "\n",
            "최종 attention weight : [[[9.6180429e-06]\n",
            "  [8.6220843e-04]\n",
            "  [3.5097243e-03]\n",
            "  [9.5622498e-01]\n",
            "  [1.9150067e-03]\n",
            "  [3.5099234e-02]\n",
            "  [1.4689063e-03]\n",
            "  [2.9138316e-06]\n",
            "  [1.7087453e-04]\n",
            "  [7.3649467e-04]]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9k4pmjkg1hT"
      },
      "source": [
        "# 양방향 LSTM과 어텐션 메커니즘 (IMDB 리뷰데이터)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOhZCnsGg54Q"
      },
      "source": [
        "from tensorflow.keras.datasets import imdb\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sb0aaImuhd2V",
        "outputId": "39205431-3e7a-4bb4-fe7f-84a1a2242d30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "vocab_size = 10000\n",
        "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "<string>:6: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:155: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/datasets/imdb.py:156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "  x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gyCQVyzhkKU",
        "outputId": "da7ecfd2-faa6-4bb7-a5b2-2fdbe07e234d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print('리뷰의 최대 길이 :', max(len(line) for line in x_train))\n",
        "print('리뷰의 평균 길이 :', sum(map(len, x_train)) / len(x_train))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "리뷰의 최대 길이 : 2494\n",
            "리뷰의 평균 길이 : 238.71364\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIJOATDYiDJ5"
      },
      "source": [
        "max_len = 500\n",
        "x_train_padding = pad_sequences(x_train, maxlen=max_len)\n",
        "x_test_padding = pad_sequences(x_test, maxlen=max_len)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btgjh58Kib9r"
      },
      "source": [
        "## 바다나우 어텐션 (Bahdanau Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z6ux4avGikgS"
      },
      "source": [
        "* attention score를 얻는 식\n",
        "$$ score(query, key) = W^T tanh(W_1 key + W_2 query) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CULQvwT0iZum"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwBGR0XAi0XD"
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = Dense(units)\n",
        "        self.W2 = Dense(units)\n",
        "        self.V = Dense(1)\n",
        "    \n",
        "\n",
        "    def call(self, values, query):\n",
        "        '''\n",
        "        shape 정리\n",
        "            query: (batch_size, hidden_size)\n",
        "\n",
        "            hidden with time axis: (batch_size, 1, hidden_size)\n",
        "            at_score: (batch_size, max_length, 1)\n",
        "            at_weight: (batch_size, max_length, 1)\n",
        "            context_v: (batch_size, hidden_size)\n",
        "        '''\n",
        "\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        at_score = self.V(tf.nn.tanh(self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        at_weight = tf.nn.softmax(at_score, axis=1)\n",
        "\n",
        "        context_v = tf.reduce_sum(at_weight * values, axis=1)\n",
        "\n",
        "        return context_v, at_weight\n",
        "    "
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frBHk8-OkN4t"
      },
      "source": [
        "## 양방향 LSTM + 어텐션 메커니즘"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y11QBK0OkMpt"
      },
      "source": [
        "from tensorflow.keras.layers import Dense, Embedding, Bidirectional, LSTM, Concatenate, Dropout\n",
        "from tensorflow.keras import Input, Model\n",
        "from tensorflow.keras import optimizers\n",
        "import os"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRhj34lmkew_"
      },
      "source": [
        "sequence_input = Input(shape=(max_len,), dtype='int32')\n",
        "\n",
        "embedded_sequences = Embedding(vocab_size, 128, input_length=max_len, mask_zero=True)(sequence_input)\n",
        "lstm = Bidirectional(LSTM(64, dropout=0.5, return_sequences=True))(embedded_sequences)\n",
        "lstm, forward_h, forward_c, backward_h, backward_c = Bidirectional(LSTM(64, dropout=0.5, return_sequences=True, return_state=True))(lstm)\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kaXBQJzCk3td",
        "outputId": "d8f81c15-711e-47b0-e54a-e8c1ec9ef83c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(lstm.shape, forward_h.shape, forward_c.shape, backward_h.shape, backward_c.shape)\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(None, 500, 128) (None, 64) (None, 64) (None, 64) (None, 64)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkQcCyr1lsBl"
      },
      "source": [
        "state_h = Concatenate()([forward_h, backward_h])\n",
        "state_c = Concatenate()([forward_c, backward_c])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BthJeul4l26D"
      },
      "source": [
        "attention = BahdanauAttention(64)\n",
        "context_vector, attention_weights = attention(lstm, state_h)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2OcSmGF3l9FY"
      },
      "source": [
        "dense1 = Dense(20, activation='relu')(context_vector)\n",
        "dropout = Dropout(0.5)(dense1)\n",
        "output = Dense(1, activation='sigmoid')(dropout)\n",
        "model = Model(inputs=sequence_input, outputs=output)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FfHtdUDAmVBc"
      },
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M_Yu8zaOnZNg",
        "outputId": "c08aa937-c68a-4506-ee78-ae9e62333239",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "history = model.fit(x_train_padding, y_train, epochs=5, batch_size=256,\n",
        "                    validation_data=(x_test_padding, y_test), verbose=1)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5\n",
            "98/98 [==============================] - 644s 6s/step - loss: 0.4683 - accuracy: 0.7651 - val_loss: 0.2885 - val_accuracy: 0.8800\n",
            "Epoch 2/5\n",
            "98/98 [==============================] - 619s 6s/step - loss: 0.2421 - accuracy: 0.9134 - val_loss: 0.2951 - val_accuracy: 0.8787\n",
            "Epoch 3/5\n",
            "98/98 [==============================] - 621s 6s/step - loss: 0.1858 - accuracy: 0.9365 - val_loss: 0.3058 - val_accuracy: 0.8771\n",
            "Epoch 4/5\n",
            "98/98 [==============================] - 613s 6s/step - loss: 0.1570 - accuracy: 0.9471 - val_loss: 0.3428 - val_accuracy: 0.8664\n",
            "Epoch 5/5\n",
            "98/98 [==============================] - 605s 6s/step - loss: 0.1249 - accuracy: 0.9599 - val_loss: 0.3936 - val_accuracy: 0.8733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "562nltr2ok9S",
        "outputId": "c2fd198a-3cea-4878-a809-aedb21f83c79",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "print(\"\\n 테스트 정확도: %.4f\" % (model.evaluate(x_test_padding, y_test)[1]))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "782/782 [==============================] - 409s 523ms/step - loss: 0.3936 - accuracy: 0.8733\n",
            "\n",
            " 테스트 정확도: 0.8733\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEYlUt-f4t8S"
      },
      "source": [
        "# seq2seq with attention 스페인-영어 번역기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBDf0fq755nS"
      },
      "source": [
        "## 데이터 준비하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BYNkDd4b02ut"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import matplotlib.ticker as ticker\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import time\n",
        "import re\n",
        "import os\n",
        "import io"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3-NNcvT5IU4"
      },
      "source": [
        "path_to_zip = tf.keras.utils.get_file('spa-eng.zip', extract=True,\n",
        "                                      origin='http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OFLVbTce5inj",
        "outputId": "f7763c7b-b6fa-429b-9591-a21d8c62521b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "path_to_file = os.path.dirname(path_to_zip) + '/spa-eng/spa.txt'\n",
        "\n",
        "with open(path_to_file, 'r') as f:\n",
        "    raw = f.read().splitlines()\n",
        "\n",
        "print('Data Size :', len(raw))\n",
        "print('Example :')\n",
        "\n",
        "for sen in raw[0:100][::20]:\n",
        "    print('>>', sen)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Data Size : 118964\n",
            "Example :\n",
            ">> Go.\tVe.\n",
            ">> Wait.\tEsperen.\n",
            ">> Hug me.\tAbrázame.\n",
            ">> No way!\t¡Ni cagando!\n",
            ">> Call me.\tLlamame.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KKsOb7vN6LRR"
      },
      "source": [
        "## 데이터 전처리 : 정제하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZN34Mvk-5q-c"
      },
      "source": [
        "def preprocess_sentence(raw_sentence, s_token=False, e_token=False):\n",
        "    sentence = raw_sentence.lower().strip()\n",
        "\n",
        "    sentence = re.sub(r'([?.!,])', r' \\1 ', sentence)\n",
        "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
        "    sentence = re.sub(r'[^a-zA-Z?.!,]+', ' ', sentence)\n",
        "    sentence = sentence.strip()\n",
        "\n",
        "    if s_token:\n",
        "        sentence = '<start> ' + sentence\n",
        "    if e_token:\n",
        "        sentence = sentence + ' <end>'\n",
        "\n",
        "    return sentence\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "byE2_nDG67cK",
        "outputId": "395394a9-2f8c-4b9a-e75d-d6de311c681c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "encode_corpus = []\n",
        "decode_corpus = []\n",
        "\n",
        "num_examples = 30000\n",
        "\n",
        "for pair in raw[:num_examples]:\n",
        "    eng, spa = pair.split('\\t')\n",
        "\n",
        "    encode_corpus.append(preprocess_sentence(eng))\n",
        "    decode_corpus.append(preprocess_sentence(spa, s_token=True, e_token=True))\n",
        "\n",
        "print('English :', encode_corpus[100])\n",
        "print('Spanish :', decode_corpus[100])\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English : go away !\n",
            "Spanish : <start> salga de aqu ! <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fRyflDI8y0M"
      },
      "source": [
        "## 데이터 전처리 : 토큰화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bhAHqS_m77lQ"
      },
      "source": [
        "def tokenize(corpus):\n",
        "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    tokenizer.fit_on_texts(corpus)\n",
        "\n",
        "    tensor = tokenizer.texts_to_sequences(corpus)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')\n",
        "\n",
        "    return tensor, tokenizer"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IT1vvfni9GmX",
        "outputId": "a00938fa-d335-4d91-b882-fc970bd45b94",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# 토큰화 하기\n",
        "encode_tensor, encode_tokenizer = tokenize(encode_corpus)\n",
        "decode_tensor, decode_tokenizer = tokenize(decode_corpus)\n",
        "\n",
        "# 훈련데이터와 검증데이터로 분리하기\n",
        "encode_train, encode_val, decode_train, decode_val = \\\n",
        "    train_test_split(encode_tensor, decode_tensor, test_size=0.2)\n",
        "\n",
        "print('English Vocab size :', len(encode_tokenizer.index_word))\n",
        "print('Spanish Vocab size :', len(decode_tokenizer.index_word))\n",
        "print()\n",
        "print('train length :', len(encode_train))\n",
        "print('valid length :', len(encode_val))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "English Vocab size : 4931\n",
            "Spanish Vocab size : 8893\n",
            "\n",
            "train length : 24000\n",
            "valid length : 6000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WqNEWKy69MfN"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.w_decode = tf.keras.layers.Dense(units)\n",
        "        self.w_encode = tf.kears.layers.Dense(units)\n",
        "        self.w_combine = tf.kears.layers.Dense(1)\n",
        "    \n",
        "    def call(self, h_encode, h_decode):\n",
        "        '''\n",
        "        h_encode : (batch, length, units)\n",
        "        h_decode : (batch, units)\n",
        "        '''\n",
        "        wh_encode = self.w_encode(h_encode)\n",
        "        wh_decode = self.w_decode(tf.expand_dims(h_decode, 1))\n",
        "\n",
        "        # attention score\n",
        "        at_score = self.w_com(tf.nn.tanh(wh_decode + wh_encode))\n",
        "\n",
        "        # attention distribution(weight)\n",
        "        at_weight = tf.nn.softmax(at_score, axis=1)\n",
        "\n",
        "        # context vector\n",
        "        context_v = at_weight * wh_encode\n",
        "        context_v = tf.reduce_sum(context_v, axis=1)\n",
        "\n",
        "        return context_v, at_weight\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hB6th7NS_zPw"
      },
      "source": [
        "![](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-P-2.max-800x600.jpg)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dKAYarTn9O27"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim, encode_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = tf.kears.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(encode_units)\n",
        "    \n",
        "    def call(self, encode_input):\n",
        "\n",
        "        embedded_input = self.embedding(encode_input)\n",
        "        encode_output = self.lstm(embedded_input)\n",
        "\n",
        "        # shape 디버깅\n",
        "        print('input shape :', encode_input.shape)\n",
        "        print('after embedding ->', embedded_input.shape)\n",
        "        print('after lstm ->', encode_output.shape)\n",
        "\n",
        "        return encode_output\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dAG7hiXA7_k"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}