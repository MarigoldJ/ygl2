{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "20210618_nlp_day7.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPOwaox5dInCVifdYb44gt0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarigoldJ/ygl2/blob/main/class/20210618_nlp_day7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiqqFQh8rjFM"
      },
      "source": [
        "# Today's Topic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1VMtqbY-sDns"
      },
      "source": [
        "* RNN의 문제점\n",
        "    * 기울기 소실\n",
        "    * 번역에는 사용하기 힘들다?\n",
        "        * 각 나라별 어순 정보 파악 힘들다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05_77ktatp94"
      },
      "source": [
        "# Seq2seq 구현하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pP0FX47ztsQS"
      },
      "source": [
        "## LSTM Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vP2Uj4OGeEAP"
      },
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIkXifezt5nV"
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    '''\n",
        "    seq2seq의 encoder\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, encoder_units):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(encoder_units)\n",
        "        # return_sequences 매개변수를 기본값 False로 전달\n",
        "    \n",
        "    def call(self, x):      # __call__ 과 비슷한 듯\n",
        "        '''\n",
        "        x를 넣고 중간에 데이터의 shape을 디버깅\n",
        "        '''\n",
        "        print('입력할 때 shape :', x.shape)\n",
        "\n",
        "        e = self.embedding(x)\n",
        "        print(\"Embedding Layer를 거친 뒤 shape :\", e.shape)\n",
        "\n",
        "        output_v = self.lstm(e)\n",
        "        print(\"LSTM Layer를 거친 뒤 shape :\", output_v.shape)\n",
        "\n",
        "        return output_v\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hfyWzV7bvNHk",
        "outputId": "875165f9-0aa1-4445-c43a-08b33fc9a9b0"
      },
      "source": [
        "vocab_size = 30000\n",
        "emb_size = 256\n",
        "lstm_size = 512\n",
        "batch_size = 1\n",
        "sample_seq_len = 3\n",
        "\n",
        "print('Vocab Size :', vocab_size)\n",
        "print('Embedding Size :', emb_size)\n",
        "print('LSTM Size :', lstm_size)\n",
        "print('Batch Size :', batch_size)\n",
        "print('Sample Sequence Length :', sample_seq_len)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size : 30000\n",
            "Embedding Size : 256\n",
            "LSTM Size : 512\n",
            "Batch Size : 1\n",
            "Sample Sequence Length : 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1-z_A15vlR2",
        "outputId": "597fb388-6b00-434a-8c95-9b5e65164c69"
      },
      "source": [
        "encoder = Encoder(vocab_size, emb_size, lstm_size)\n",
        "sample_encoder_input = tf.zeros((batch_size, sample_seq_len))\n",
        "\n",
        "sample_encoder_output = encoder(sample_input)\n",
        "# 인코더 LSTM의 최종 State (이후 컨벡스트 벡터로 사용될 예정)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력할 때 shape : (1, 3)\n",
            "Embedding Layer를 거친 뒤 shape : (1, 3, 256)\n",
            "LSTM Layer를 거친 뒤 shape : (1, 512)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_G6hD8VHwPOX"
      },
      "source": [
        "![](https://aiffelstaticprd.blob.core.windows.net/media/images/GN-4-L-6.max-800x600.jpg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bsgn-HGcwPwO"
      },
      "source": [
        "## LSTM Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n1gQ-uhQv-Tf"
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    '''\n",
        "    seq2seq의 decoder\n",
        "    '''\n",
        "    def __init__(self, vocab_size, embedding_dim, decoder_units):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = tf.keras.layers.LSTM(decoder_units, return_sequences=True)\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        self.softmax = tf.keras.layers.Softmax(axis=-1)\n",
        "\n",
        "    def call(self, x, context_v):\n",
        "        '''\n",
        "        디코더의 입력 x와 인코더의 컨벡스트 벡터(final state)를 인자로 받음\n",
        "        중간의 데이터들의 shape을 디버깅\n",
        "        '''\n",
        "        print('입력할 때 shape :', x.shape)\n",
        "\n",
        "        e = self.embedding(x)\n",
        "        print(\"Embedding Layer를 거친 뒤 shape :\", e.shape)\n",
        "\n",
        "        # 컨벡스트 벡터의 브로드캐스팅, 확장\n",
        "        context_vh = tf.repeat(tf.expand_dims(context_v, axis=1), repeats=x.shape[1], axis=1)\n",
        "        eh = tf.concat([e, context_vh], axis=-1)\n",
        "        print('Context Vector가 합쳐진 shape :', eh.shape)\n",
        "\n",
        "        output_v = self.lstm(eh)\n",
        "        print(\"LSTM Layer를 거친 뒤 shape :\", output_v.shape)\n",
        "\n",
        "        output = self.fc(output_v)\n",
        "        print('Decoder 최종 output의 shape :', output.shape)\n",
        "\n",
        "        return output"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OF7N4UGypVh",
        "outputId": "77e4ff63-e482-4408-911a-2270ec2b6cfa"
      },
      "source": [
        "vocab_size = 30000\n",
        "emb_size = 256\n",
        "lstm_size = 512\n",
        "batch_size = 1\n",
        "sample_seq_len = 3\n",
        "\n",
        "print('Vocab Size :', vocab_size)\n",
        "print('Embedding Size :', emb_size)\n",
        "print('LSTM Size :', lstm_size)\n",
        "print('Batch Size :', batch_size)\n",
        "print('Sample Sequence Length :', sample_seq_len)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vocab Size : 30000\n",
            "Embedding Size : 256\n",
            "LSTM Size : 512\n",
            "Batch Size : 1\n",
            "Sample Sequence Length : 3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmDOdHH_ytom",
        "outputId": "a54e4c85-0b60-41fb-8879-29f52c29571e"
      },
      "source": [
        "decoder = Decoder(vocab_size, emb_size, lstm_size)\n",
        "sample_decoder_input = tf.zeros((batch_size, sample_seq_len))\n",
        "\n",
        "sample_decoder_output = decoder(sample_decoder_input, sample_encoder_output)\n",
        "# Decoder.call(x, context_v)를 호출\n"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "입력할 때 shape : (1, 3)\n",
            "Embedding Layer를 거친 뒤 shape : (1, 3, 256)\n",
            "Context Vector가 합쳐진 shape : (1, 3, 768)\n",
            "LSTM Layer를 거친 뒤 shape : (1, 3, 512)\n",
            "Decoder 최종 output의 shape : (1, 3, 30000)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj7RG4NK4nC8"
      },
      "source": [
        "# 어텐션 메커니즘 (Attention Mechanism)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3sf4xAN84xxt"
      },
      "source": [
        "* 기존의 RNN에 기반한 seq2seq 모델의 문제점\n",
        "    1. 기억 소실, 기울기 소실\n",
        "    2. 하나의 고정된 벡터에 모든 정보를 압축하려다 보니 정보 손실이 발생\n",
        "* 어텐션 아이디어\n",
        "    * 디코더에서 출력단어를 예측하는 매 시점마다, 인코더에서의 *전체* 입력 문장을 다시 한번 참고함\n",
        "    * 전체 입력 문장을 모두 동일한 비율로 참고하는 것이 아니라, 해당 시점에 예측할 단어와 연관성 있는 단어 부분을 중점적으로 참고함"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hCJkJkQ-z66J",
        "outputId": "cdbf9423-cdd2-4f45-9140-ba23e21bf12f"
      },
      "source": [
        "dictionary = {'2017': 'Transformer', '2018': 'BERT'}\n",
        "\n",
        "print(dictionary['2017'])\n",
        "print(dictionary['2018'])"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Transformer\n",
            "BERT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cu8sTLl06gRr"
      },
      "source": [
        "![](https://wikidocs.net/images/page/22893/%EC%BF%BC%EB%A6%AC.PNG)\n",
        "* Query : t 시점의 디코더 셀에서의 hidden state\n",
        "* Key : 모든 시점의 인코더 셀에서의 hidden state\n",
        "* Value : 모든 시점의 인코더 셀의 hidden state"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "azS6xR0E6guU"
      },
      "source": [
        "# 닷 프로덕트 어텐션 (Dot-Product Attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVDAjnmmClER"
      },
      "source": [
        "## 이론 설명"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u63w2r8t65-D"
      },
      "source": [
        "![](https://wikidocs.net/images/page/22893/dotproductattention1_final.PNG)\n",
        "\n",
        "그림 설명\n",
        "* 디코더에서 sos, je를 넣어 je, suis를 예측한 뒤, 세번째 단어를 예측하려는 시점\n",
        "* suis 다음 단어를 예측하기 위해서, 인코더의 hidden state들을 참조하여 다음 단어 etudiant를 예측해내는 모습"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jPHixQVP7jdy"
      },
      "source": [
        "### 1.Attention Score 구하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AUhOsQBD8qPa"
      },
      "source": [
        "![](https://wikidocs.net/images/page/22893/dotproductattention2_final.PNG)\n",
        "\n",
        "\n",
        "그림 설명\n",
        "* 디코더의 현재 시점 t에서의 state와, 인코더의 모든 시점에서의 state의 유사도를 점수화한다.(내적을 통해 유사도를 계산하는 듯)\n",
        "* 수식은 아래와 같다.\n",
        "$$ score(s_t, h_i) = s_t^T h_i $$\n",
        "$$ e^t = [ score(s_t, h_1), score(s_t, h_2), ..., score(s_t, h_N) ] $$\n",
        "    * $s_t$는 디코더의 현재 시점 t에서의 state (행벡터)\n",
        "    * $h_i$는 인코더의 모든 시점 중 특정 시점 i에서의 state (행벡터)\n",
        "    * $e^t$는 디코더의 현재 시점 t에서의 attention score 모음값\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "URodiYSY-cdS"
      },
      "source": [
        "### 2.Attention Distribution 구하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nnXEwfUS-p7i"
      },
      "source": [
        "* 소프트맥스(softmax) 함수를 통해 어텐션 분포를 구한다\n",
        "\n",
        "![](https://wikidocs.net/images/page/22893/dotproductattention3_final.PNG)\n",
        "\n",
        "그림 설명\n",
        "* 1에서 계산한 각 시점별 Attention Score를 softmax 함수로 처리한다.\n",
        "* 수식은 아래와 같다.\n",
        "$$ a^t = softmax(e^t) $$\n",
        "    * $a^t$는 어텐션 가중치 모음값인 attention distribution\n",
        "    * $e^t$는 디코더의 현재 시점 t에서의 attention score 모음값"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eAVuwZB9ASMB"
      },
      "source": [
        "### 3.Attention Value 구하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8ntOqy-ASbC"
      },
      "source": [
        "* 각 인코더의 어텐션 가중치와 은닉 상태를 가중합해서 어텐션 값(attention value)을 구한다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/22893/dotproductattention4_final.PNG)\n",
        "\n",
        "그림 설명\n",
        "* 2에서 계산한 각 시점별 가중치인 Attention distribution과, 각 시점별 state를 곱하여 Attention value를 구한다.\n",
        "    * 이 결과 값은 종합적인 정보를 담은 state라고 생각할 수 있다.\n",
        "    * 인코더에서 디코더로 전달하는 state를 만든 것이다.\n",
        "        * 기존 : 인코더의 마지막 state\n",
        "        * 현재 하는 것 : 인코더의 state를 종합적으로 고려한 새로운 state\n",
        "* 수식은 아래와 같다.\n",
        "$$ a_t = \\sum^{N}_{i=1}{a^t_i h_i} $$\n",
        "    * $a^t$는 어텐션 가중치 모음값인 attention distribution\n",
        "        * 해당 시점을 얼마나 반영할지 나타낸다고 생각하기\n",
        "    * $h_i$는 인코더의 특정 시점 i에서의 state (행벡터)\n",
        "    * $a_t$는 가중치가 반영된 state (행벡터)\n",
        "        * $h_i$ 들을 대표하는 state라고 생각하면 좋을 듯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8Na9oSYASnR"
      },
      "source": [
        "### 4.Attention value, decoder state 연결하기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YdZg7liDorm"
      },
      "source": [
        "* 3에서 구한 어텐션 값과, 기존의 디코더의 현재 시점 t의 state를 연결한다.\n",
        "* 새로운 벡터를 만드는 것이다.\n",
        "\n",
        "![](https://wikidocs.net/images/page/22893/dotproductattention5_final_final.PNG)\n",
        "\n",
        "그림 설명\n",
        "* 3에서 계산한 인코더의 종합 State인 Attention Value와, 기존의 디코더의 현재 시점 t의 state를 연결하여 새로운 벡터를 만든다.\n",
        "* 굳이 수식으로 쓰면 아래와 같다.\n",
        "$$ v_t = [a_t;s_t] $$\n",
        "    * $v_t$는 $a_t$와 $s_t$를 단순히 연결한 것. (행벡터)\n",
        "    * $a_t$는 가중치가 반영된 인코더의 state (행벡터)\n",
        "        * 인코더의 정보를 종합한 state로 보면 좋을 듯!\n",
        "    * $s_t$는 디코더의 현재 시점 t에서의 state (행벡터)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h13mALIoDl94"
      },
      "source": [
        "### 5.출력층 연산의 입력 벡터 만들기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0VBWO4vDpbX"
      },
      "source": [
        "* 논문 내용에 따르면, 4에서의 $v_t$가 출력층으로 가기전에 한가지 연산을 거침.\n",
        "* 연산이라 함은 $tanh$ 함수를 거치는 연산을 의미함.\n",
        "\n",
        "![](https://wikidocs.net/images/page/22893/st.PNG)\n",
        "\n",
        "그림 설명\n",
        "* 어텐션 메커니즘을 사용하기 전에는, 출력층의 입력으로 $s_t$(디코더에서 시점 t의 state) 가 사용됨.\n",
        "* 어텐션 메커니즘을 사용하는 지금은, 출력층의 입력으로 $\\tilde{s_t}$(attention이 반영된 디코더에서 시점 t의 state) 가 사용됨.\n",
        "* $\\tilde{s_t}$를 구하는 수식은 아래와 같다.\n",
        "$$ \\tilde{s_t}=tanh(W_c v_t + b_c) $$\n",
        "    * $v_t$는 $a_t$와 $s_t$를 단순히 연결한 것. (행벡터)\n",
        "    * $W_c$는 학습 가능한 가중치 행렬\n",
        "    * $b_c$는 편향\n",
        "    * $\\tilde{s_t}$는 attention이 반영된 새로운 $s_t$(디코더의 현재 시점 t에서의 state) (행벡터)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZxlXXH6DnXX"
      },
      "source": [
        "### 6.출력층으로부터 예측 벡터 얻기"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D100tnfdDqDn"
      },
      "source": [
        "* 5에서 구한 출력층의 입력벡터를 출력층에 넣고 연산하여, 예측 벡터($\\tilde{y_t}$)를 얻는다.\n",
        "\n",
        "* 수식은 아래와 같다. \n",
        "$$ \\tilde{y_t}=softmax(W_y \\tilde{s_t} + b_y) $$\n",
        "    * $\\tilde{s_t}$는 attention이 반영된 새로운 $s_t$(디코더의 현재 시점 t에서의 state) (행벡터)\n",
        "    * $W_y$는 학습 가능한 가중치 행렬\n",
        "    * $b_y$는 편향\n",
        "    * $\\tilde{y_t}$는 예측벡터 (이 값을 문자로 바꾸면 예측 문자가 된다)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfqId8aydWG5"
      },
      "source": [
        "# Bahdanau Attention 구현"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_hz9aUZdMN9"
      },
      "source": [
        "* attention score를 얻는 식\n",
        "$$ score_{alignment} = W \\bullet tanh(W_{decoder} H_{decoder} + W_{encoder} H_{encoder}) $$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0q_U1lWjCrg_"
      },
      "source": [
        "class BahdanauAttention(tf.keras.layer.Layer):    \n",
        "\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.w_decoder = tf.keras.layers.Dense(units)\n",
        "        self.w_encoder = tf.keras.layers.Dense(units)\n",
        "        self.w_combine = tf.keras.layers.Dense(1)\n",
        "    \n",
        "    def call(self, h_encoder, h_decoder):\n",
        "        '''\n",
        "        h_encoder : encoder hidden state\n",
        "        h_decoder : decoder hidden state\n",
        "        '''\n",
        "        wh_encoder = self.w_encoder(h_encoder)\n",
        "        wh_decoder = self.w_decoder(tf.expand_dims(h_decoder, 1))\n",
        "\n",
        "        print('[h_encoder] shape :', h_encoder.shape)\n",
        "        print('[w_encoder x h_encoder] shape :', wh_encoder.shape)\n",
        "        \n",
        "        print('[h_decoder] shape :', h_decoder.shape)\n",
        "        print('[w_decoder x h_decoder] shape :', wh_decoder.shape)\n",
        "\n",
        "        # attention score\n",
        "        at_score = self.w_combine(tf.nn.tanh(wh_decoder + wh_encoder))\n",
        "        print('[score_alignment] shape :', at_score.shape)\n",
        "\n",
        "        # attention distribution(weight)\n",
        "        at_weight = tf.nn.softmax(at_score, axis=1)\n",
        "        print('\\n최종 attention weight :', at_weight.numpy())\n",
        "\n",
        "        # attention value?(convext vector 얻기)\n",
        "        context_v = at_weight * wh_decoder\n",
        "        context_v = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_v, at_weight\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXHJ4HXm6MvO"
      },
      "source": [
        "w_size = 100\n",
        "\n",
        "print(f'Hidden State를 {w_size}차원으로 Mapping\\n')\n",
        "\n",
        "### 아래 미완성\n",
        "###############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G06Nhl4ZDlh_"
      },
      "source": [
        "# Luong Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cl3LMAm4D4B5"
      },
      "source": [
        "## 이론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PmwdpwoD4u_"
      },
      "source": [
        "## 코드 구현"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz2_-pIoD9ax"
      },
      "source": [
        "class LuongAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, units):\n",
        "        super(LuongAttention, self).__init__()\n",
        "        self.W_combine = tf.keras.layers.Dense(units)\n",
        "\n",
        "    def call(self, )\n",
        "    ### 아래 미완성\n",
        "    ###############"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9k4pmjkg1hT"
      },
      "source": [
        "# 양방향 LSTM과 어텐션 메커니즘 (IMDB 리뷰데이터)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DOhZCnsGg54Q"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}